{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarmenTheodoraCraciun/HairClasification/blob/main/CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importarea biblotecilor si clonarea repo-ului"
      ],
      "metadata": {
        "id": "eK0jhh-3j70l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "q_EQ9jfWgUcM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./HairClasification\n",
        "!git clone https://github.com/CarmenTheodoraCraciun/HairClasification.git\n",
        "!mv ./HairClasification/originalData/Straight/ ./HairClasification/originalData/straight\n",
        "!mv ./HairClasification/originalData/Wavy/ ./HairClasification/originalData/wavy"
      ],
      "metadata": {
        "id": "I60MDMr2iz7O",
        "outputId": "7639ce5d-5358-4f87-ab24-e4b02bcf970a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HairClasification'...\n",
            "remote: Enumerating objects: 6487, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 6487 (delta 16), reused 69 (delta 9), pack-reused 6397 (from 3)\u001b[K\n",
            "Receiving objects: 100% (6487/6487), 413.76 MiB | 31.33 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "Updating files: 100% (2060/2060), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing\n"
      ],
      "metadata": {
        "id": "q0ztLKXqjcst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_image(file_path):\n",
        "    try:\n",
        "        img = cv2.imread(file_path)\n",
        "        return img is not None\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def resize_image(img, size):\n",
        "    \"\"\"\n",
        "    Resizes an image using bilinear interpolation.\n",
        "\n",
        "    Args:\n",
        "        img: Input image.\n",
        "        size: Desired size of the output image (width, height).\n",
        "\n",
        "    Returns:\n",
        "        Resized image.\n",
        "    \"\"\"\n",
        "    original_height, original_width, _ = img.shape\n",
        "    new_width, new_height = size\n",
        "    resized_img = np.zeros((new_height, new_width, 3), dtype=np.uint8)\n",
        "\n",
        "    for i in range(new_width):\n",
        "        for j in range(new_height):\n",
        "            #i, j = pixel in the resized image\n",
        "            # x, y = pixel in the original image\n",
        "            x = i * (original_width - 1) / (new_width - 1)\n",
        "            y = j * (original_height - 1) / (new_height - 1)\n",
        "\n",
        "            # Neighborhood values\n",
        "            x0 = int(np.floor(x))\n",
        "            x1 = min(x0 + 1, original_width - 1)\n",
        "            y0 = int(np.floor(y))\n",
        "            y1 = min(y0 + 1, original_height - 1)\n",
        "\n",
        "            # Extract the intensity values ​​of neighbors\n",
        "            Ia = img[y0, x0] # stanga sus\n",
        "            Ib = img[y0, x1] # drepata sus\n",
        "            Ic = img[y1, x0] # stanga jos\n",
        "            Id = img[y1, x1] # dreapta jos\n",
        "\n",
        "            # Calculates the weight of each neighboring to the final value\n",
        "            wa = (x1 - x) * (y1 - y)\n",
        "            wb = (x - x0) * (y1 - y)\n",
        "            wc = (x1 - x) * (y - y0)\n",
        "            wd = (x - x0) * (y - y0)\n",
        "\n",
        "            # The final value of the new pixel\n",
        "            pixel = wa * Ia + wb * Ib + wc * Ic + wd * Id\n",
        "            resized_img[j, i] = np.round(pixel).astype(int)\n",
        "\n",
        "    return resized_img\n",
        "\n",
        "def augment_image(img, size=(128, 128), shear_range=0.1, zoom_range=0.1, horizontal_flip=True):\n",
        "    \"\"\"\n",
        "    Preprocesses and augments an image by applying resizing, shearing, zooming, and horizontal flipping.\n",
        "\n",
        "    Args:\n",
        "        img: Input image.\n",
        "        size: Desired size of the output image (width, height).\n",
        "        shear_range: Range for random shearing.\n",
        "        zoom_range: Range for random zooming.\n",
        "        horizontal_flip: Whether to randomly flip the image horizontally.\n",
        "\n",
        "    Returns:\n",
        "        Preprocessed and augmented image.\n",
        "    \"\"\"\n",
        "    rows, cols, ch = img.shape\n",
        "    shear_factor = np.random.uniform(-shear_range, shear_range)\n",
        "    M_shear = np.array([[1, shear_factor, 0], [0, 1, 0]], dtype=np.float32)\n",
        "    img = cv2.warpAffine(img, M_shear, (cols, rows))\n",
        "\n",
        "    zoom_factor = np.random.uniform(1 - zoom_range, 1 + zoom_range)\n",
        "    img = cv2.resize(img, None, fx=zoom_factor, fy=zoom_factor)\n",
        "\n",
        "    if zoom_factor < 1: # zoom out\n",
        "        new_height, new_width = img.shape[:2]\n",
        "        # Number of pixels added to padding (black) and its addition\n",
        "        pad_height = (rows - new_height) // 2\n",
        "        pad_width = (cols - new_width) // 2\n",
        "        img = cv2.copyMakeBorder(img, pad_height, pad_height,\n",
        "            pad_width, pad_width, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    else: # zoom in\n",
        "        # Position of the starting point for cutting\n",
        "        start_x = (img.shape[1] - cols) // 2\n",
        "        start_y = (img.shape[0] - rows) // 2\n",
        "        img = img[start_y:start_y + rows, start_x:start_x + cols]\n",
        "\n",
        "    if horizontal_flip and np.random.random() < 0.5:\n",
        "        img = cv2.flip(img, 1)\n",
        "\n",
        "    img = cv2.resize(img, size)\n",
        "\n",
        "    return img\n",
        "\n",
        "def preprocess_images(input_dir, output_dir, size=(128, 128), augment_prob=0.3):\n",
        "    \"\"\"\n",
        "    Resizes images to the specified size and saves them to the output directory.\n",
        "    Applies augmentations to a random subset of images.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing the input images.\n",
        "        output_dir: Directory to save the preprocessed images.\n",
        "        size: Desired size of the output images (width, height).\n",
        "        augment_prob: Probability of applying augmentation to each image.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Start processing data.\")\n",
        "    for category in os.listdir(input_dir):\n",
        "        category_dir = os.path.join(input_dir, category)\n",
        "        output_category_dir = os.path.join(output_dir, category)\n",
        "        if not os.path.exists(output_category_dir):\n",
        "            os.makedirs(output_category_dir)\n",
        "\n",
        "        num_images = 0\n",
        "        if os.path.isdir(category_dir):\n",
        "            for idx, img_name in enumerate(os.listdir(category_dir)):\n",
        "                img_path = os.path.join(category_dir, img_name)\n",
        "\n",
        "                if is_image(img_path):\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is not None:\n",
        "                        if np.random.rand() < augment_prob:\n",
        "                            img = augment_image(img, size=size)\n",
        "                        else:\n",
        "                            img = resize_image(img, size)\n",
        "                        new_img_name = f\"{category}_{idx}.png\"\n",
        "                        cv2.imwrite(os.path.join(output_category_dir, new_img_name), img)\n",
        "                        num_images += 1\n",
        "                    else:\n",
        "                        print(f\"Failed to load image: {img_path}\")\n",
        "                else:\n",
        "                    print(f\"Not an image: {img_path}\")\n",
        "\n",
        "        print(f\"Folder {output_category_dir} has {num_images} images.\")\n",
        "\n",
        "    print(\"The data are processed.\")"
      ],
      "metadata": {
        "id": "V8nJ3kZsGMhL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_images('./HairClasification/originalData', './HairClasification/processData', size=(128, 128), augment_prob=0.3)"
      ],
      "metadata": {
        "id": "EwsjNADHIYMU",
        "outputId": "cb4e8697-bcd1-4280-90e2-f7b3e64fbb4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start processing data.\n",
            "Folder ./HairClasification/processData/straight has 530 images.\n",
            "Folder ./HairClasification/processData/dreadlocks has 443 images.\n",
            "Folder ./HairClasification/processData/wavy has 331 images.\n",
            "Not an image: ./HairClasification/originalData/curly/rs_1080x1080-200330130638-1080-ariana-grande-curly-hair-instagram-am-033020.gif\n",
            "Folder ./HairClasification/processData/curly has 515 images.\n",
            "Folder ./HairClasification/processData/kinky has 232 images.\n",
            "The data are processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading data with labels"
      ],
      "metadata": {
        "id": "klQddslMJEte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_and_labels(input_dir, size=(128, 128)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for category in os.listdir(input_dir):\n",
        "        category_dir = os.path.join(input_dir, category)\n",
        "        if os.path.isdir(category_dir):\n",
        "            for img_name in os.listdir(category_dir):\n",
        "                img_path = os.path.join(category_dir, img_name)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, size)\n",
        "                    images.append(img)\n",
        "                    labels.append(category)\n",
        "    return np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "MLTDRoJwh2Tg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = './HairClasification/processData'\n",
        "images, labels = load_images_and_labels(input_dir)"
      ],
      "metadata": {
        "id": "yBkS02LLkxoG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Augmentation and Normalization\n",
        "\n",
        "* Normalization - to contain only 0 and 1 values"
      ],
      "metadata": {
        "id": "bVg21Bs6lEP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Apply augmentation to all images in the dataset\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for img, label in zip(images, labels):\n",
        "    img = img.astype('float32') / 255.0  # Normalize the image\n",
        "    img = img[np.newaxis, ...]  # Expand dimensions for datagen flow\n",
        "    gen = datagen.flow(img, batch_size=1)\n",
        "    # Generate a few augmented images per original image (e.g., 5 augmentations each)\n",
        "    for _ in range(5):\n",
        "        aug_img = next(gen)[0]  # Correct usage of next() function\n",
        "        augmented_images.append(aug_img)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "\n",
        "images_augmented = np.array(augmented_images)\n",
        "labels_augmented = np.array(augmented_labels)"
      ],
      "metadata": {
        "id": "53JYsReblCzz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    images_augmented,\n",
        "    labels_augmented,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "krU20qP4lAZv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##One-hot Encoding\n",
        "\n",
        "* One-hot encoding is a technique used to convert categorical data into a numeric format that a machine learning model can understand\n",
        "  * e.g. [0,0,0,1,0] means that the hair is straight.\n"
      ],
      "metadata": {
        "id": "19PK89Y7la86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_test_categorical = to_categorical(y_test_encoded)"
      ],
      "metadata": {
        "id": "TsqPotSVldyn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pregatirea datelor de antrenament si validare"
      ],
      "metadata": {
        "id": "OYd75cYylnsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = datagen.flow(X_train, y_train_categorical, batch_size=32)\n",
        "validation_datagen = ImageDataGenerator().flow(X_test, y_test_categorical, batch_size=32)"
      ],
      "metadata": {
        "id": "7XwWKBj5lmsY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Construirea modelului"
      ],
      "metadata": {
        "id": "3LlzM5B6lzSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incarcarea in modelul preantrenat VGG16\n",
        "\n",
        "VGG16 este un model de rețea neurală convoluțională (CNN) binecunoscut și puternic, dezvoltat de echipa de cercetare de la Visual Geometry Group (VGG) de la Universitatea din Oxford. Numele său, VGG16, derivă din structura sa de 16 straturi adânci de rețea neurală."
      ],
      "metadata": {
        "id": "jxLO5v09ltX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "sDV0s41qlxvA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Construirea peste VGG16"
      ],
      "metadata": {
        "id": "eWa8UJjdopro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))"
      ],
      "metadata": {
        "id": "eRsBvHf4l1--"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compilarea si antrenarea"
      ],
      "metadata": {
        "id": "4fvMPFwnl3cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "098b6_JPl79z"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_datagen, epochs=20, validation_data=validation_datagen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P80vgiyEpqi4",
        "outputId": "1df319fb-5917-424b-a01c-6a3b42bb9004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m204/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m5:37\u001b[0m 6s/step - accuracy: 0.4009 - loss: 2.3956"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluarea modelului"
      ],
      "metadata": {
        "id": "xPGluVqyl-0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, accuracy, 'b', label='Accuracy on training data')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Accuracy on validation data')\n",
        "plt.title('Accuracy during training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EYYpr686mCuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test_normalized, y_test_categorical)\n",
        "print(f'Test accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "t1LsjnI1mA1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_encoded = model.predict(X_test_normalized)\n",
        "y_pred = np.argmax(y_pred_encoded, axis=1)\n",
        "y_test = np.argmax(y_test_categorical, axis=1)\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "eBjSeTiJmGA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "conf_df = pd.DataFrame(conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_df, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.title(\"Confuzion matrix\")\n",
        "plt.ylabel(\"Real classes\")\n",
        "plt.xlabel(\"Predicted classes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_PC6zR2imHm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}